# Changelog v0.0.12

**Release Date**: 2026-01-01  
**Type**: Feature Enhancement - Core Feature Improvements

## ðŸŽ¯ Core Feature Improvements

### 1. âœ… Fixed Auto-Tuning Persistence
**Issue**: Optimal HPA targets weren't being saved to database properly, showing `null` in API.

**Fix**:
- Rewrote `update_optimal_target()` with proper error handling
- Added explicit INSERT/UPDATE logic instead of INSERT OR REPLACE
- Added verification after save to confirm data persistence
- Added detailed logging for debugging
- Proper rollback on errors

**Impact**:
- Auto-tuning now works correctly
- Optimal targets persist across restarts
- Learning progress is saved and tracked
- Dashboard shows actual optimal targets

**Before**:
```json
{
  "optimal_target": null
}
```

**After**:
```json
{
  "optimal_target": 70,
  "confidence": 0.82,
  "samples": 156,
  "last_updated": "2026-01-01T13:51:09"
}
```

---

### 2. ðŸ“Š Lowered Pattern Detection Requirements
**Issue**: Pattern detection required 100+ samples, taking too long to detect patterns.

**Changes**:
- **Minimum samples**: 100 â†’ 20 (5x faster detection)
- **CPU samples**: 50 â†’ 10 (5x faster)
- Added "learning" state messages
- Added confidence scoring based on sample count
- Better logging to show progress

**Confidence Levels**:
- < 20 samples: 30% confidence (learning)
- 20-50 samples: 60% confidence (medium)
- 50-100 samples: 80% confidence (good)
- 100+ samples: 95% confidence (high)

**Impact**:
- Patterns detected in ~30 minutes instead of 2-3 hours
- Users see pattern detection progress
- Faster adaptation to workload characteristics
- Better user experience with progress indicators

**Before**:
```
WARNING - Insufficient data for pattern detection (45 points)
Pattern: unknown
```

**After**:
```
INFO - Learning pattern (45/20 samples collected)
INFO - Pattern analysis: samples=45, mean=0.075, std=0.012, cv=0.16
INFO - Detected pattern: steady (confidence: 60%)
```

---

### 3. ðŸ–¥ï¸ Fixed Node CPU Usage Queries
**Issue**: Cluster monitoring showed 0% CPU usage for nodes.

**Root Cause**: Single query approach failed when node_exporter metrics weren't available.

**Fix**:
- Added fallback query strategy
- **Primary**: `node_cpu_seconds_total` (node_exporter)
- **Fallback**: `container_cpu_usage_seconds_total` (cAdvisor)
- Same approach for memory metrics
- Better error logging to identify which source works

**Impact**:
- Cluster monitoring now shows actual CPU/memory usage
- Works with different Prometheus setups
- Better visibility into cluster resource utilization
- Priority-based scaling can make better decisions

**Before**:
```json
{
  "nodes": [{
    "name": "orbstack",
    "cpu_usage": 0,
    "memory_usage_gb": 0
  }]
}
```

**After**:
```json
{
  "nodes": [{
    "name": "orbstack",
    "cpu_usage": 2.5,
    "memory_usage_gb": 8.2
  }]
}
```

---

## ðŸ“ Technical Details

### Auto-Tuning Database Fix

**Old Code** (problematic):
```python
def update_optimal_target(self, deployment: str, target: int, confidence: float):
    self.conn.execute("""
        INSERT OR REPLACE INTO optimal_targets
        (deployment, optimal_target, confidence, samples_count, last_updated)
        VALUES (?, ?, ?, 
                COALESCE((SELECT samples_count FROM optimal_targets WHERE deployment = ?), 0) + 1,
                ?)
    """, (deployment, target, confidence, deployment, datetime.now()))
    self.conn.commit()
```

**New Code** (fixed):
```python
def update_optimal_target(self, deployment: str, target: int, confidence: float):
    try:
        cursor = self.conn.execute("""
            SELECT optimal_target, confidence, samples_count 
            FROM optimal_targets WHERE deployment = ?
        """, (deployment,))
        
        existing = cursor.fetchone()
        
        if existing:
            new_samples = existing[2] + 1
            self.conn.execute("""
                UPDATE optimal_targets
                SET optimal_target = ?, confidence = ?, samples_count = ?, last_updated = ?
                WHERE deployment = ?
            """, (target, confidence, new_samples, datetime.now(), deployment))
        else:
            self.conn.execute("""
                INSERT INTO optimal_targets
                (deployment, optimal_target, confidence, samples_count, last_updated)
                VALUES (?, ?, ?, 1, ?)
            """, (deployment, target, confidence, datetime.now()))
        
        self.conn.commit()
        
        # Verify the save
        cursor = self.conn.execute("""
            SELECT optimal_target, confidence, samples_count 
            FROM optimal_targets WHERE deployment = ?
        """, (deployment,))
        
        verified = cursor.fetchone()
        if not verified:
            logger.error(f"Failed to verify optimal target save!")
            
    except Exception as e:
        logger.error(f"Error updating optimal target: {e}", exc_info=True)
        self.conn.rollback()
```

### Pattern Detection Thresholds

| Metric | Old | New | Improvement |
|--------|-----|-----|-------------|
| Min samples | 100 | 20 | 5x faster |
| Min CPU samples | 50 | 10 | 5x faster |
| Detection time | 2-3 hours | 30 minutes | 4-6x faster |
| User feedback | None | Progress messages | Better UX |

### Node Metrics Query Strategy

```python
# Primary query (node_exporter)
cpu_usage_query = f'sum(rate(node_cpu_seconds_total{{mode!="idle",instance=~".*{node_name}.*"}}[5m]))'

# Fallback query (cAdvisor)
cpu_usage_query_fallback = f'sum(rate(container_cpu_usage_seconds_total{{node="{node_name}",container!="",container!="POD"}}[5m]))'
```

---

## ðŸš€ Performance Impact

### Auto-Tuning
- âœ… Targets now persist correctly
- âœ… Learning accumulates over time
- âœ… Confidence increases with more samples
- âœ… Dashboard shows real data

### Pattern Detection
- âš¡ 5x faster detection (30 min vs 2-3 hours)
- ðŸ“Š Progress visibility for users
- ðŸŽ¯ Confidence scoring
- ðŸ’¡ Better user experience

### Cluster Monitoring
- ðŸ“ˆ Actual CPU/memory usage displayed
- ðŸ”„ Fallback queries for reliability
- ðŸŽ¯ Better priority-based decisions
- ðŸ“Š Complete cluster visibility

---

## ðŸ”§ Migration Notes

### Database
- No schema changes required
- Existing data compatible
- Auto-tuning will start working immediately

### Configuration
- No configuration changes needed
- Works with existing Prometheus setups
- Automatic fallback for metrics

### Behavior Changes
- Pattern detection happens faster (good!)
- More log messages during learning phase
- Auto-tuning targets will start appearing in API

---

## âœ… Testing

### Auto-Tuning
```bash
# Check if optimal target is saved
curl http://localhost:5000/api/deployment/demo/demo-app/optimal

# Should show:
{
  "optimal_target": 70,
  "confidence": 0.82,
  "samples": 156
}
```

### Pattern Detection
```bash
# Check AI insights
curl http://localhost:5000/api/ai/insights/demo-app | jq '.patterns'

# Should show pattern within 30 minutes of deployment
```

### Cluster Monitoring
```bash
# Check cluster metrics
curl http://localhost:5000/api/cluster/metrics | jq '.nodes[0].cpu_usage'

# Should show non-zero value
```

---

## ðŸ“Š Metrics

### Before v0.0.12
- Auto-tuning: âŒ Not persisting
- Pattern detection: â±ï¸ 2-3 hours
- Node CPU usage: âŒ Showing 0%

### After v0.0.12
- Auto-tuning: âœ… Working, persisting
- Pattern detection: âš¡ 30 minutes
- Node CPU usage: âœ… Showing actual values

---

## ðŸ”— Related

- v0.0.11: Added cluster monitoring
- v0.0.11-v2: Fixed namespace filter
- v0.0.11-v5: Fixed cluster metrics response format
- v0.0.12: Fixed core features (THIS RELEASE)

---

**Upgrade Path**: Deploy v0.0.12 â†’ Core features work immediately

**Breaking Changes**: None - fully backward compatible

**Recommended**: Yes - fixes critical auto-tuning feature
